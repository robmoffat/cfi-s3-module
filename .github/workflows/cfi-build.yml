name: CFI Build

permissions:
  id-token: write
  contents: read

on:
  workflow_dispatch:
  push:
    branches: [main]

jobs:
  prepare-matrix:
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.set-matrix.outputs.matrix }}
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Generate matrix from config files
        id: set-matrix
        run: |
          TARGETS_DIR=config
          matrix='{"target": []}'

          for target_file in $TARGETS_DIR/*.json; do
            if [[ "$target_file" == *".json" ]]; then
              id=$(jq -r '.id' "$target_file")
              name=$(jq -r '.name' "$target_file")
              path=$(jq -r '.path' "$target_file")
              provider=$(jq -r '.provider' "$target_file")
              service=$(jq -r '.service' "$target_file")
              
              target_json=$(jq -c '.' "$target_file")
              matrix=$(echo "$matrix" | jq --argjson target "$target_json" '.target += [$target]')
            fi
          done

          echo "matrix={\"target\": $(echo "$matrix" | jq -c '.target')}" >> $GITHUB_OUTPUT
          echo "Generated matrix with $(echo "$matrix" | jq '.target | length') targets"

  deploy-and-scan:
    needs: prepare-matrix
    runs-on: ubuntu-latest
    strategy:
      max-parallel: 1
      matrix:
        target: ${{ fromJson(needs.prepare-matrix.outputs.matrix).target }}
      fail-fast: false

    env:
      AWS_REGION: ${{ secrets.AWS_REGION }}
      OUTPUT_DIR: results
      TARGET_ID: ${{ matrix.target.id }}
      TARGET_NAME: ${{ matrix.target.name }}
      TARGET_PATH: ${{ matrix.target.path }}
      TARGET_PROVIDER: ${{ matrix.target.provider }}
      TARGET_SERVICE: ${{ matrix.target.service }}
      # Terraform variables
      TF_VAR_project_id: ${{ secrets.GCP_PROJECT_ID }}
      TF_VAR_target_id: ${{ matrix.target.id }}
      TF_VAR_github_run_id: ${{ github.run_id }}
      TF_VAR_github_run_number: ${{ github.run_number }}
      TF_VAR_github_actor: ${{ github.actor }}
      TF_VAR_github_repository: ${{ github.repository }}
      TF_VAR_github_workflow: ${{ github.workflow }}
      TF_VAR_github_sha: ${{ github.sha }}

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Configure AWS Credentials (OIDC)
        if: matrix.target.provider == 'aws'
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/TerraformRole
          aws-region: ${{ env.AWS_REGION }}

      - name: Configure Azure Credentials (OIDC)
        if: matrix.target.provider == 'azure'
        uses: azure/login@v1
        with:
          client-id: ${{ secrets.AZURE_CLIENT_ID }}
          tenant-id: ${{ secrets.AZURE_TENANT_ID }}
          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}

      - name: Authenticate to Google Cloud (OIDC)
        if: matrix.target.provider == 'gcp'
        uses: google-github-actions/auth@v1
        with:
          token_format: "id_token"
          id_token_audience: "https://iam.googleapis.com/projects/${{ secrets.GCP_PROJECT_NUMBER }}/locations/global/workloadIdentityPools/finos-oidc-pool/providers/github-provider"
          workload_identity_provider: "projects/${{ secrets.GCP_PROJECT_NUMBER }}/locations/global/workloadIdentityPools/finos-oidc-pool/providers/github-provider"
          service_account: "gha-deployer@${{ secrets.GCP_PROJECT_ID }}.iam.gserviceaccount.com"

      - name: Set up Terraform
        uses: hashicorp/setup-terraform@v3

      - name: Install Steampipe
        run: |
          echo "üì¶ Installing Steampipe..."
          sudo /bin/sh -c "$(curl -fsSL https://steampipe.io/install/steampipe.sh)"
          steampipe --version
          
          echo "üì¶ Installing Steampipe plugins..."
          steampipe plugin install aws
          steampipe plugin install azure
          steampipe plugin install gcp
          
          echo "üöÄ Starting Steampipe service..."
          steampipe service start
          
          echo "‚úÖ Steampipe installed and configured successfully"

      - name: Install aws-nuke
        if: matrix.target.provider == 'aws'
        run: |
          wget -q https://github.com/rebuy-de/aws-nuke/releases/download/v2.25.0/aws-nuke-v2.25.0-linux-amd64.tar.gz
          tar -xzf aws-nuke-v2.25.0-linux-amd64.tar.gz
          sudo mv aws-nuke-v2.25.0-linux-amd64 /usr/local/bin/aws-nuke
          sudo chmod +x /usr/local/bin/aws-nuke

      - name: Cleanup AWS resources from previous runs
        if: matrix.target.provider == 'aws'
        continue-on-error: true
        run: |
          echo "üßπ Cleaning up AWS resources from previous runs using aws-nuke..."

          # Create aws-nuke config with generic tag-based filtering
          cat > nuke-config.yml << EOF
          regions:
            - ${{ env.AWS_REGION }}
            - global

          account-blocklist:
            - "999999999999" # Prevent accidental deletion in wrong account

          accounts:
            "${{ secrets.AWS_ACCOUNT_ID }}":
              # Global filter: only delete resources with Environment=cfi-test tag
              resource-types:
                # Use __global__ to apply filter to ALL resource types
                excludes:
                  - IAMUser  # Don't delete IAM users for safety
                  - IAMGroup # Don't delete IAM groups for safety
                  - S3Object # Don't delete individual objects (will delete buckets)
              
              filters:
                # Generic tag-based filter for all taggable resources
                __global__:
                  - property: tag:Environment
                    value: "cfi-test"
                
                # Name-based filters for resources that don't support tags
                S3Bucket:
                  - type: glob
                    value: "*terraform-*"
                  - type: glob
                    value: "*cfi-test*"
                
                IAMRole:
                  - type: glob
                    value: "terraform-*"
                  - type: glob
                    value: "*cfi-test*"
                
                CloudFormationStack:
                  - type: glob
                    value: "terraform-*"
                  - type: glob
                    value: "*cfi-test*"
                
                CloudWatchLogsLogGroup:
                  - type: glob
                    value: "*terraform*"
                  - type: glob
                    value: "*cfi-test*"
          EOF

          # Verify AWS credentials are working
          echo "üîê Verifying AWS credentials..."
          aws sts get-caller-identity || {
            echo "‚ùå AWS credentials not configured properly"
            exit 1
          }

          # Run aws-nuke in dry-run mode first to see what would be deleted
          echo "üìã Dry-run to see what would be deleted:"
          aws-nuke -c nuke-config.yml --no-dry-run=false || true

          # Actually delete resources (with force flag)
          echo "üí• Deleting resources:"
          aws-nuke -c nuke-config.yml --no-dry-run --force || true

          echo "‚úÖ AWS cleanup completed"

      - name: Cleanup Azure resources from previous runs
        if: matrix.target.provider == 'azure'
        continue-on-error: true
        run: |
          echo "üßπ Cleaning up Azure resources from previous runs..."

          # Delete resource groups with cfi-test or terraform prefix
          for rg in $(az group list --query "[?contains(name, 'cfi-test') || contains(name, 'terraform-')].name" -o tsv); do
            echo "Deleting resource group: $rg"
            az group delete --name $rg --yes --no-wait || true
          done

          echo "‚úÖ Azure cleanup completed"

      - name: Cleanup GCP resources from previous runs
        if: matrix.target.provider == 'gcp'
        continue-on-error: true
        run: |
          echo "üßπ Cleaning up GCP resources from previous runs..."

          PROJECT_ID="${{ secrets.GCP_PROJECT_ID }}"
          LABEL_FILTER="labels.environment=cfi-test"
          NAME_FILTER="name:cfi-test OR name:terraform-"

          # Generic cleanup function
          cleanup() {
            local cmd=$1
            echo "Running: $cmd"
            eval "$cmd" || true
          }

          # Compute resources (by label)
          cleanup "gcloud compute instances list --filter='$LABEL_FILTER' --format='value(name,zone)' | while read name zone; do gcloud compute instances delete \$name --zone=\$zone --quiet; done"
          cleanup "gcloud compute disks list --filter='$LABEL_FILTER' --format='value(name,zone)' | while read name zone; do gcloud compute disks delete \$name --zone=\$zone --quiet; done"

          # Storage (by name pattern - GCS buckets don't support labels well)
          cleanup "gcloud storage buckets list --filter='$NAME_FILTER' --format='value(name)' | while read bucket; do gcloud storage rm --recursive gs://\$bucket || true; gcloud storage buckets delete gs://\$bucket || true; done"

          # Networking (by name pattern - often don't have labels)
          cleanup "gcloud compute networks list --filter='$NAME_FILTER' --format='value(name)' | while read name; do gcloud compute networks delete \$name --quiet; done"

          # Serverless (by label)
          cleanup "gcloud functions list --filter='$LABEL_FILTER' --format='value(name,region)' | while read name region; do gcloud functions delete \$name --region=\$region --quiet; done"
          cleanup "gcloud run services list --filter='metadata.labels.environment=cfi-test' --format='value(metadata.name,metadata.namespace)' | while read name region; do gcloud run services delete \$name --region=\$region --quiet; done"

          # Databases (by label)  
          cleanup "gcloud sql instances list --filter='settings.userLabels.environment=cfi-test' --format='value(name)' | while read name; do gcloud sql instances delete \$name --quiet; done"
          cleanup "bq ls --filter='$LABEL_FILTER' --format=json | jq -r '.[].id' | while read dataset; do bq rm -r -f -d \$dataset; done"

          # Messaging (by label)
          cleanup "gcloud pubsub topics list --filter='$LABEL_FILTER' --format='value(name)' | while read topic; do gcloud pubsub topics delete \$topic --quiet; done"
          cleanup "gcloud pubsub subscriptions list --filter='$LABEL_FILTER' --format='value(name)' | while read sub; do gcloud pubsub subscriptions delete \$sub --quiet; done"

          echo "‚úÖ GCP cleanup completed"

      - name: Setup provider configuration with tagging
        run: |
          echo "üìù Copying provider configuration for $TARGET_PROVIDER..."

          # GCP requires lowercase labels - sanitize target_id
          if [ "$TARGET_PROVIDER" = "gcp" ]; then
            export TF_VAR_target_id_sanitized=$(echo "$TARGET_ID" | tr '[:upper:]' '[:lower:]' | tr '_' '-')
            export TF_VAR_github_run_id_sanitized=$(echo "${{ github.run_id }}" | tr '[:upper:]' '[:lower:]')
            echo "üìù Sanitized for GCP: target_id=$TF_VAR_target_id_sanitized"
          fi

          # Copy the appropriate provider.tf.example to the target directory
          if [ -f "remote/$TARGET_PROVIDER/provider.tf.example" ]; then
            cp "remote/$TARGET_PROVIDER/provider.tf.example" "$TARGET_PATH/provider.tf"
            echo "‚úÖ Copied remote/$TARGET_PROVIDER/provider.tf.example to $TARGET_PATH/provider.tf"
          else
            echo "‚ö†Ô∏è  No provider.tf.example found for $TARGET_PROVIDER"
          fi

          # Show what was copied
          if [ -f "$TARGET_PATH/provider.tf" ]; then
            echo "Provider configuration:"
            cat "$TARGET_PATH/provider.tf"
          fi

      - name: Cache Python Poetry virtualenv
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/pypoetry
            ~/.virtualenvs
            prowler/.venv
          key: prowler-${{ hashFiles('prowler/poetry.lock') }}
          restore-keys: prowler-

      - name: Install Prowler and Python deps
        run: |
          git clone -b PRWLR-7257-map-ccc-objects-in-aws-provider --single-branch https://github.com/prowler-cloud/prowler
          cd prowler
          pipx install poetry
          poetry install

      - name: Terraform Apply
        id: terraform-apply
        run: |
          set -euo pipefail

          echo "üöÄ Deploying module: $TARGET_NAME (id: $TARGET_ID)"
          echo "üìÅ Path: $TARGET_PATH"
          echo "‚òÅÔ∏è  Provider: $TARGET_PROVIDER"
          echo "üîß Service: $TARGET_SERVICE"

          mkdir -p "$OUTPUT_DIR"

          cd "$GITHUB_WORKSPACE/$TARGET_PATH"
          
          if [ "$TARGET_PROVIDER" = "aws" ]; then
            terraform init
            terraform apply -auto-approve -input=false
          elif [ "$TARGET_PROVIDER" = "azure" ]; then
            export ARM_SUBSCRIPTION_ID="${{ secrets.AZURE_SUBSCRIPTION_ID }}"
            terraform init -upgrade
            terraform apply -auto-approve -input=false
          elif [ "$TARGET_PROVIDER" = "gcp" ]; then
            export TF_VAR_project_id="${{ secrets.GCP_PROJECT_ID }}"
            terraform init
            terraform apply -auto-approve -input=false
          fi

          echo "‚úÖ Terraform apply completed successfully for $TARGET_ID"

      - name: Run Prowler Scan
        id: prowler-scan
        if: success() && steps.terraform-apply.outcome == 'success'
        continue-on-error: true
        run: |
          set -euo pipefail

          echo "üîç Running Prowler scan for $TARGET_ID..."
          cd "$GITHUB_WORKSPACE/prowler"

          if [ "$TARGET_PROVIDER" = "aws" ]; then
            poetry run python prowler-cli.py aws
          elif [ "$TARGET_PROVIDER" = "azure" ]; then
            poetry run python prowler-cli.py azure --az-cli-auth
          elif [ "$TARGET_PROVIDER" = "gcp" ]; then
            poetry run python prowler-cli.py gcp
          fi

          echo "‚úÖ Prowler scan completed successfully for $TARGET_ID"

      - name: Copy OCSF Results
        if: always() && steps.prowler-scan.outcome != 'skipped'
        run: |
          cd "$GITHUB_WORKSPACE/prowler"
          
          if ls output/*.ocsf.json 1> /dev/null 2>&1; then
            latest_file=$(ls -t output/*.ocsf.json | head -n 1)
            if [ -n "$latest_file" ] && [ -f "$latest_file" ]; then
              cp "$latest_file" "../$OUTPUT_DIR/${TARGET_ID}.ocsf.json"
              echo "‚úÖ Successfully captured OCSF output for $TARGET_ID"
            else
              echo "‚ö†Ô∏è  No valid OCSF file found for $TARGET_ID"
            fi
          else
            echo "‚ö†Ô∏è  No OCSF output files found for $TARGET_ID"
          fi

      - name: Terraform Destroy
        if: always() && steps.terraform-apply.outcome == 'success'
        run: |
          echo "üß® Running terraform destroy for $TARGET_ID..."
          cd "$GITHUB_WORKSPACE/$TARGET_PATH"

          if [ "$TARGET_PROVIDER" = "aws" ]; then
            terraform destroy -auto-approve -input=false || true
          elif [ "$TARGET_PROVIDER" = "azure" ]; then
            terraform destroy -auto-approve -input=false || true
          elif [ "$TARGET_PROVIDER" = "gcp" ]; then
            terraform destroy -auto-approve -input=false || true
          fi

          echo "‚úÖ Terraform destroy completed for $TARGET_ID"

      - name: Upload module results
        uses: actions/upload-artifact@v4
        with:
          name: cfi-results-${{ matrix.target.id }}
          path: |
            ${{ env.OUTPUT_DIR }}
            config/${{ matrix.target.id }}.json
